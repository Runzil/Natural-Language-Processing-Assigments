{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZrAUx57vsM"
      },
      "source": [
        "Practical 1: Sentiment Detection in Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4kXPMhyngZW"
      },
      "source": [
        "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task: how might one go about classifying the texts? You will write\n",
        "code that decides whether a movie review conveys positive or\n",
        "negative sentiment.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002).\n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. introduced the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Pang et al.'s system. If you have questions about it,\n",
        "you should resolve you doubts as soon as possible with your TA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7errgRASzZ"
      },
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "While we inserted code cells to indicate where you should implement your own code, please feel free to add/remove code blocks where you see fit (but make sure that the general structure of the assignment is preserved). Also, please keep in mind that it is always good practice to structure your code properly, e.g., by implementing separate classes and functions that can be reused. **Make sure you run all your code before submitting the notebook, and do not leave unnecessary print statements / cells in your notebook that are not intended for the grader.**\n",
        "\n",
        "## Environment\n",
        "\n",
        "All code should be written in **Python 3**.\n",
        "This is the default in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaZnxptMJiD7",
        "outputId": "a4f8505d-188c-492d-bb62-2f9154381f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZyIF7lJnGn"
      },
      "source": [
        "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download).\n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects.\n",
        "\n",
        "\n",
        "**Learning Python 3**\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hok-BFu9lGoK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWyGHwE-ieQ"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "**Download the sentiment lexicon and the movie reviews dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm-rakqtlMOT",
        "outputId": "3174da95-cc75-42dc-ea9d-cbddc30c4ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-13 12:42:57--  https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662577 (647K) [text/plain]\n",
            "Saving to: ‘sent_lexicon’\n",
            "\n",
            "sent_lexicon        100%[===================>] 647.05K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2024-11-13 12:42:58 (116 MB/s) - ‘sent_lexicon’ saved [662577/662577]\n",
            "\n",
            "--2024-11-13 12:42:58--  https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 83503869 (80M) [text/plain]\n",
            "Saving to: ‘reviews.json’\n",
            "\n",
            "reviews.json        100%[===================>]  79.63M   206MB/s    in 0.4s    \n",
            "\n",
            "2024-11-13 12:42:58 (206 MB/s) - ‘reviews.json’ saved [83503869/83503869]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download sentiment lexicon\n",
        "!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
        "# download review data\n",
        "!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPwuHp5LSuQ"
      },
      "source": [
        "**Load the movie reviews.**\n",
        "\n",
        "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "careEKj-mRpl",
        "outputId": "96818e7c-fec2-4801-a9a7-0900bdd944cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of reviews: 2000 \n",
            "\n",
            "0 NEG 29\n",
            "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
            "1 NEG 11\n",
            "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
            "2 NEG 24\n",
            "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
            "3 NEG 19\n",
            "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
            "4 NEG 38\n",
            "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
            "\n",
            "Number of word types: 47743\n",
            "Number of word tokens: 1512359\n",
            "\n",
            "Most common tokens:\n",
            "         , :    77842\n",
            "       the :    75948\n",
            "         . :    59027\n",
            "         a :    37583\n",
            "       and :    35235\n",
            "        of :    33864\n",
            "        to :    31601\n",
            "        is :    25972\n",
            "        in :    21563\n",
            "        's :    18043\n",
            "        it :    15904\n",
            "      that :    15820\n",
            "     -rrb- :    11768\n",
            "     -lrb- :    11670\n",
            "        as :    11312\n",
            "      with :    10739\n",
            "       for :     9816\n",
            "       his :     9542\n",
            "      this :     9497\n",
            "      film :     9404\n"
          ]
        }
      ],
      "source": [
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences,\n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "\n",
        "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 4:\n",
        "    break\n",
        "\n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "\n",
        "print(\"\\nNumber of word types:\", len(c))\n",
        "print(\"Number of word tokens:\", sum(c.values()))\n",
        "\n",
        "print(\"\\nMost common tokens:\")\n",
        "for token, count in c.most_common(20):\n",
        "  print(\"%10s : %8d\" % (token, count))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PWaEoh8B34"
      },
      "source": [
        "#(1) Lexicon-based approach (3.5pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTSMb6ma4E8"
      },
      "source": [
        "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative, or a score from 0 to 5).\n",
        "\n",
        "In this practical, you will use the sentiment\n",
        "lexicon released by Wilson et al. (2005).\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
        "\n",
        "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogq0Eq2hQglh",
        "outputId": "059f4a50-3d51-47bd-ce5c-fdc44607bcb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
          ]
        }
      ],
      "source": [
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  line_cnt = 0\n",
        "  for line in f:\n",
        "    print(line.strip())\n",
        "    line_cnt += 1\n",
        "    if line_cnt > 4:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml4nOtIUBhn"
      },
      "source": [
        "Lexica such as this can be used to solve\n",
        "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
        "$S_{binary}$ by counting how many words have a positive or a\n",
        "negative label in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
        "\n",
        "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{negative} & \\text{otherwise}\n",
        "        \\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFnMvbeeZrc"
      },
      "source": [
        "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED2aTEYutW1-"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "reviews_words = []\n",
        "for review in reviews:\n",
        "  review_words = []\n",
        "  for sentence in review[\"content\"]:\n",
        "    review_words.extend([word for word, tag in sentence])\n",
        "  reviews_words.append(review_words)\n",
        "\n",
        "def load_sentiment_lexicon(file_path,flag):\n",
        "    lexicon = {}\n",
        "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            # Extract word and priorpolarity directly from the line\n",
        "            word = line.split(\"word1=\")[-1].split()[0]\n",
        "            prior_polarity = line.split(\"priorpolarity=\")[-1].strip()\n",
        "            type_value = line.split()[0].split('=')[1]  # Get the value after 'type='\n",
        "\n",
        "            if flag == None:\n",
        "              # Map prior polarity to numerical values\n",
        "              if prior_polarity == \"positive\":\n",
        "                  sentiment = 1\n",
        "              elif prior_polarity == \"negative\":\n",
        "                  sentiment = -1\n",
        "              else:\n",
        "                  sentiment = 0\n",
        "\n",
        "            if flag != None:\n",
        "              if prior_polarity == \"positive\" and type_value == \"strongsubj\":\n",
        "                  sentiment = 2  # Add 2 for strong type\n",
        "              elif prior_polarity == \"positive\" and type_value == \"weaksubj\":\n",
        "                  sentiment = 1\n",
        "              elif prior_polarity == \"negative\" and type_value == \"strongsubj\":\n",
        "                  sentiment = -2\n",
        "              elif prior_polarity == \"negative\" and type_value == \"weaksubj\":\n",
        "                  sentiment = -1\n",
        "              else:\n",
        "                  sentiment = 0\n",
        "\n",
        "            lexicon[word] = sentiment\n",
        "    return lexicon\n",
        "\n",
        "def get_sentiment_list(words, file_path, flag=None):\n",
        "    lexicon = load_sentiment_lexicon(file_path,flag)\n",
        "    return [lexicon.get(word, 0) for word in words]  # Default to 0 if word not found\n",
        "\n",
        "\n",
        "sentiments = []\n",
        "for k in range(len(reviews)):\n",
        "  sentiments.append(get_sentiment_list(reviews_words[k], \"sent_lexicon\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l19EJEY87AG"
      },
      "outputs": [],
      "source": [
        "sentiment_sum = []\n",
        "classifications = []\n",
        "\n",
        "for k in range(len(reviews)):\n",
        "  sentiment_sum.append(sum(sentiments[k]))\n",
        "  if sum(sentiments[k]) > 8:\n",
        "    classifications.append(1)\n",
        "  else:\n",
        "    classifications.append(-1)\n",
        "\n",
        "# print(sentiment_sum)\n",
        "# print(classifications)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOp0u-c688NB"
      },
      "outputs": [],
      "source": [
        "# Create a list to store sentiments ground truth\n",
        "GT = []\n",
        "\n",
        "# Open the JSON file line by line\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  for i, r in enumerate(reviews):\n",
        "    if r[\"sentiment\"] == \"POS\":\n",
        "      GT.append(1)\n",
        "    elif r[\"sentiment\"] == \"NEG\":\n",
        "      GT.append(-1)\n",
        "\n",
        "# print(GT)\n",
        "\n",
        "token_results = [None] * 2000\n",
        "for k in range(len(reviews)):\n",
        "  if GT[k] == classifications[k]:\n",
        "    token_results[k] = 1\n",
        "  else:\n",
        "    token_results[k] = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy528EUTphz5",
        "outputId": "a1cc68d6-0d0e-4592-e964-694028340cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.679500\n"
          ]
        }
      ],
      "source": [
        "# token_results should be a list of binary indicators; for example [1, 0, 1, ...]\n",
        "# where 1 indicates a correct classification and 0 an incorrect classification.\n",
        "token_results = token_results\n",
        "token_accuracy = sum(token_results)/len(token_results)\n",
        "print(\"Accuracy: %2f\" % token_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twox0s_3eS0V"
      },
      "source": [
        "As the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Make sure you define an appropriate threshold for this approach.\n",
        "\n",
        "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG3hUDnPtkhS"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "sentiments2 = []\n",
        "\n",
        "for k in range(len(reviews)):\n",
        "  sentiments2.append(get_sentiment_list(reviews_words[k], \"sent_lexicon\",\"flagged\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9hGzLIK9A99",
        "outputId": "f4de818a-b981-4f59-9bd0-2d7264dc9d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[19, 9, 9, 0, 1, -13, -13, -19, -29, 0, 6, 2, -28, -1, 2, -12, -9, -1, 25, 7, 2, -7, -11, 4, -4, 29, 17, -21, 14, -2, -14, 12, -4, 24, -25, -3, 5, -7, -12, -2, -18, -20, 22, 5, 2, -4, -14, 3, -3, 4, 21, 16, 6, 14, -1, 3, 5, -7, 7, 31, -6, 40, -11, -17, -19, -13, -5, -3, -9, -36, 16, 13, -7, 10, -5, -5, -27, 17, -10, -7, -4, -1, -17, -7, -16, 9, -24, -18, 26, -6, 19, -8, 19, -11, 28, -2, -14, 6, -12, 2, -1, -12, -31, 38, -3, 14, -8, 47, 0, -1, 7, 3, 0, -15, 3, 14, 9, 1, -22, 9, 30, -12, 16, -5, 11, 15, 18, -4, -12, -15, -9, 18, -5, -1, 25, 26, -15, -9, 35, -10, 54, 14, 19, -3, -36, 18, 0, 1, -19, 4, -19, 16, 18, -7, -9, -18, 11, 1, -14, -1, -42, 17, -10, 17, -14, 1, 28, 15, -3, 14, -1, 1, 14, -3, -34, -16, 21, 8, -7, -4, -41, 7, 0, -4, -3, 2, 8, 7, -20, -2, -1, -7, 15, -8, 2, 6, -15, -30, 2, 8, 32, 6, 7, -20, -13, -1, 13, -3, -22, -3, 0, 10, -27, -27, 23, -3, 8, -39, 17, 13, -7, -14, -5, 11, -10, -2, -15, 14, -24, 16, 28, 17, 27, 15, -6, -1, 11, 3, 4, -7, -13, -24, 23, -5, -13, -7, 16, -7, -19, 10, 43, -3, 2, -1, 4, -8, 4, 1, 19, -13, 10, -15, 9, 7, 11, -11, 2, -31, 18, 4, 7, 2, 6, -44, 3, 18, 4, -1, 0, 29, 10, -27, 8, 13, 12, -1, 9, 7, -5, 3, 2, 9, 2, -11, 6, 11, -14, 23, -9, -16, -6, 30, 5, 35, -21, -6, 16, 34, -12, 11, 10, 5, 1, -26, 12, -5, 0, -8, -11, -20, 9, 16, 9, 13, 6, -1, -15, 16, 21, 13, 11, 11, 2, -9, 21, -14, -4, -18, 21, 5, -10, -1, 19, 1, 7, 7, 3, -15, -2, 16, -13, -3, -10, -19, 10, 5, 0, -1, 5, 0, -14, -22, -10, -5, -34, -10, 0, 5, -18, 6, 3, -6, 6, 16, -1, 1, -17, -9, 1, 0, 11, -12, 3, 4, -32, 2, 7, -5, 12, -9, 15, 31, -18, 1, 12, -15, -4, 9, -1, -19, 17, 14, 40, -35, -44, 13, 26, -14, -6, 6, -8, -12, 19, -2, -2, 20, -8, 8, 11, 32, 9, 0, 21, 8, -20, -39, -2, -18, -23, -28, 7, 15, 15, -3, -19, 3, 9, 8, -31, -3, 3, 55, 11, 14, -28, 11, -10, -16, 11, 26, -16, -17, 4, 9, -22, 7, 9, -37, -5, -14, 0, 2, -4, -4, 15, -12, 8, -11, 13, 14, 24, 5, 7, 8, -18, 1, -4, -9, -7, -1, 14, 9, 8, 24, 25, -26, 5, 13, -4, 32, 4, -13, 24, 19, 17, -7, -3, -11, -9, 10, 19, 5, 6, 19, 2, -11, -1, -19, -5, 15, 2, 37, -14, -8, 14, -3, -7, -7, 13, 18, 4, 12, -20, -2, 22, -28, -49, 8, 29, -45, 14, 16, -1, -19, 7, 4, -17, -13, 24, 7, 23, -6, -2, 11, -8, -7, 3, -11, -14, 21, 8, 18, -24, 16, -28, -3, 22, -38, 11, -14, 26, -15, 8, 10, 6, 21, -2, -13, 13, -25, 3, 36, -35, 15, 17, -1, 9, 13, -3, -20, 16, -1, -20, 10, -14, 27, -56, 2, 3, -28, 2, 10, 34, -7, 28, 0, 13, -11, -13, -5, 9, 5, 17, 4, 31, 10, 14, 3, -16, 1, 23, -3, -21, 25, 9, -18, -19, 18, 24, -24, 12, -30, -14, 0, 8, -7, -7, -5, -23, -4, -3, -5, 7, 9, 30, -8, 5, -12, 6, -18, -25, 17, 11, 9, -12, 37, -35, 16, -8, 5, 13, -1, 16, 21, 32, 13, 15, -8, -6, 18, 5, -7, 0, -17, 7, 9, 2, 10, 2, 66, 2, 5, 21, 4, 10, 3, 36, 4, -2, 4, 4, -18, -11, -1, -7, 26, 26, 23, 20, -22, 4, -10, 10, 19, -5, -33, -39, 26, -26, 6, 9, 6, -5, 3, 25, -30, -1, -6, -12, 25, 3, 23, 0, 24, -25, 15, -25, 16, 4, 13, 3, 5, 3, 6, -2, 26, -8, 16, -8, 16, 25, 6, -10, -8, 12, 52, -16, 9, 11, -13, -5, 20, 18, 6, -24, 30, -1, 16, 8, -58, 10, -5, -34, -6, -24, 5, 1, -24, -20, 1, 17, -23, 39, -9, 3, 25, -46, -7, -12, 3, -27, 13, -2, 3, 3, 19, 9, -18, 19, 3, 6, -17, 5, -7, -10, -3, 5, -14, 19, 29, -26, 3, -2, -16, -9, 38, 9, 5, -8, 3, 7, -13, -28, 12, -19, 7, 16, -68, 21, -20, 1, 7, -14, 2, 6, 18, -5, -9, 49, 2, -14, -8, 1, 24, -14, 8, 22, -3, 2, -27, -3, 22, 19, -9, 4, 1, -17, -12, 28, 8, -33, -17, 13, 27, -23, 31, 45, 42, -22, -24, -35, 11, -30, -17, 3, 8, -14, 5, 2, 5, -19, 20, 29, 3, -24, 12, -20, 9, -3, 6, -4, -14, -14, -5, -19, 10, 16, 9, -4, -21, 34, -13, 3, -2, 0, -4, 18, -1, 27, 7, -2, 7, 22, -75, 8, 12, 1, -18, -20, -13, -23, 1, -17, 26, 3, 19, 29, 7, 38, -5, -3, 5, -20, 10, -12, 12, -1, 12, -20, 4, 2, -31, -4, 12, 23, 5, 27, 9, -8, 19, -16, 2, -1, 32, 1, -4, 18, -3, -9, 5, -2, -15, 7, -9, -11, 9, 23, 25, -2, -5, 24, 3, 7, -5, 10, -32, 14, -4, -11, -15, -9, -2, 23, 11, 11, 17, 25, 12, 52, 10, 5, 11, 13, 21, -3, 5, 9, 2, 26, 16, -10, 32, -2, 9, -9, 9, -2, -2, 5, -17, 3, -14, 15, 7, 14, -55, 21, 12, 14, 8, -4, 21, 19, -11, -3, 47, 32, 46, -4, 14, -11, 58, 18, 0, 34, 57, 11, 35, 5, 7, 7, 34, 18, 24, -1, 21, 28, 59, 15, 18, 11, 52, 26, 14, 4, 39, 15, 37, 17, 32, 36, -15, 44, 30, 47, 22, 11, 29, 51, 14, 1, 33, 25, -15, 19, 45, 9, 13, 8, 6, 36, 21, 32, -4, 20, 13, 13, 4, 32, 14, 16, 79, 29, 11, 48, 6, 27, 23, 18, -26, 14, 41, 8, 75, 25, 34, 11, 29, 14, 8, 6, -13, -6, 9, 82, 2, 38, 2, -7, -18, 45, 23, 49, 46, 3, -21, 5, -18, -1, -4, 34, 7, -4, 6, 3, 61, 41, 3, 23, 6, 48, 41, -29, 46, 13, 15, -5, -2, 27, 51, 20, 19, -5, -8, 56, 13, 42, 18, 25, -4, 29, 0, 36, 65, 39, 2, 45, 41, 34, 6, 3, 20, 24, 23, 38, 41, 14, -6, 50, 0, 47, 10, 33, 117, 47, 1, 47, 15, 22, 53, -5, 14, 1, -5, 8, 18, 9, 5, 30, -13, 22, 30, 12, 43, -1, 21, 17, 21, 6, -3, 26, 38, 28, 9, 18, 39, 22, -3, 10, 43, -4, 22, -16, -19, 29, 42, 49, 1, 11, -3, -10, 53, 3, 18, -7, -2, 23, 17, 14, 27, 30, 6, -37, 27, 46, -3, 35, 27, 27, 3, 5, 55, 21, 54, 27, 35, 12, 21, 24, 3, 28, -8, 0, 22, 17, 16, 24, 45, 13, 7, -22, 21, -49, 26, 47, -7, 124, 16, -22, 19, -2, 51, -8, 31, 7, 46, 3, 4, 2, 12, 0, 45, 21, 23, 61, 42, 46, 9, 15, 39, 5, 48, 1, 12, 54, 29, 59, 5, 17, 25, 6, 24, 27, 14, 19, 11, 19, 3, 68, 32, 21, -6, -38, 11, 32, 38, 31, 21, 7, 64, 49, 44, 33, 36, 13, 34, 22, 53, 6, 14, 50, 2, 19, 5, 24, 13, 3, 105, 43, 2, 27, 33, 28, 9, 11, 2, 3, 11, 25, 4, 41, 33, 3, 7, 33, -7, 11, 50, 8, 7, 16, 26, 14, 13, -12, 51, -37, 21, 45, 13, 7, 14, 33, 17, 1, 35, 18, 16, 36, 42, 26, 25, 8, 27, 23, 27, 11, 4, -5, 14, 15, 32, 28, 65, 14, 4, 10, 15, 31, -41, 8, 12, -15, 35, 4, 7, -19, 42, 58, 1, 8, 49, 14, 10, 24, 44, 24, 3, 1, -5, 41, 26, 5, 15, 32, 59, 37, 7, 10, 16, 6, 41, 48, 7, 12, 13, 10, -16, 16, 32, -10, -10, 3, 66, 5, 15, 52, 2, 6, 54, 41, 9, 10, 25, 27, 7, -5, 7, 36, -2, 0, 29, 22, 49, 4, 11, 45, -12, 11, -8, 32, 11, -4, 48, -14, 19, 10, 2, 37, -1, 24, 20, 23, 2, 35, 44, 17, 53, 27, 3, 17, -15, 26, 10, -5, 45, 14, -8, 21, 20, 11, 12, 23, 9, 17, 6, 29, 9, -17, 17, 27, 77, -3, 59, 16, -42, 21, 23, 29, 116, 2, 54, 12, 19, 9, 19, 27, 23, 16, 50, -2, 93, 15, -3, 3, 5, 14, 13, 55, 9, 18, 35, 20, 24, -3, 20, 9, -2, 17, 22, 6, 29, 31, 35, -15, 73, 32, 3, -7, 33, 20, 41, 77, 39, 19, 16, 59, 20, 25, 6, 70, 14, 28, 38, 17, -3, 8, 49, 22, 71, 30, 41, 1, 34, 68, 25, 36, 69, 10, -5, 0, 46, 23, -12, 0, -23, 31, 37, 10, 6, 0, 1, -8, 49, 50, 1, 98, 103, 23, 19, 18, 24, 4, 38, 25, 39, 56, 19, -19, 18, 26, 31, 25, 14, 0, 12, -4, 27, 14, 20, 21, 63, 54, 46, 29, 25, -21, 47, 25, 51, 42, -24, 22, 54, 22, 21, 9, 11, 29, 23, -2, 7, 21, 8, -15, -1, 29, 13, 14, -6, 8, -33, 9, 39, 25, 4, 20, -10, -1, 14, 8, 22, 18, 66, 5, -37, 35, 17, 43, 44, 15, 22, 42, 36, 49, -7, 14, 21, 70, 32, 56, 31, 62, -8, 20, -9, -5, 35, 16, 34, 23, 32, 143, 3, 34, 37, -6, 33, -5, -22, 43, 23, 17, 19, 42, 24, 75, 43, 29, -2, 42, -20, -2, 2, 17, 12, -12, 22, 5, -3, 13, -1, 3, 26, 37, 8, 8, -5, 76, 57, 7, -1, 31, 66, 41, 11, -4, 12, 37, 30, 48, 35, 21, 37, 6, 22, 18, 8, 27, 23, 65, 10, 64, 14, 6, 41, 24, 34, 20, 14, 6, -11, 5, 9, 29, -2, 1, 11, 32, 11, 19, -4, 30, 24, -5, 22, 39, 21, 16, 29, 76, -7, 27, 58, 12, 28, -9, 4, 10, 13, 15, -2, 20, -5, 20, 14, 30, 4, 1, 5, 53, 3, 26, 63, 22, 17, 19, 54, -18, 34, -33, -6, 49, 35, 20, -13, 4, 61, 22, 25, 26, 27, 16, -7, -1, 4, 28, 0, -7, 25, 4, -9, -7, 15, 12, 20, 68, 78, -10, 42, 23, 0, -2, 31, -14, 53, 2, 25, 34, 28, 13, -9, 20, 36, 14, -3, -16, 8, 24, 4, 15, -5, 84, 10, 10, 33, -2, 19, 84, 2, 58, -4, 62, 24, -24, 27, 9, 7, 11, 4, 14, 9, 10, 34, 37, 30, 17, 25, 41, 34, -8, 12, 38, 26, 54, 1, 39, 42, 40, 31, 60, -8, 21, 41, 11, 32, 1, 24, 120, -1, 24, 29, 0, 58, 20, 24, 74, 28, -3, 29, -5, 19, 29, 13, 44, 34, 3, 6, 50, -4, 47, 34, -15, 56, 2, 41, 20, -15, 23, 16, 32, 10, 15, 11, 14, 48, 53, 1, 3, 10, 5, 53, 24, 35, 26, 44, -12, 20, 30, 5, -16, 6, 18, 39, 18, 12, 0, 110, 24, 29, 18, 26, 1, 69, -5, 69, 16, 18, 9, 49, 23, 0, 27, 21, 44, 26, -4, 8, 45, 17, 19, -3, 43, 13, 23, 74, 13, -32, 38, 30]\n"
          ]
        }
      ],
      "source": [
        "classifications2=[]\n",
        "sentiment_sum2 = []\n",
        "\n",
        "for k in range(len(reviews)):\n",
        "  sentiment_sum2.append(sum(sentiments2[k]))\n",
        "  if sum(sentiments2[k]) > 10:\n",
        "    classifications2.append(1)\n",
        "  else:\n",
        "    classifications2.append(-1)\n",
        "\n",
        "magnitude_results = [None] * 2000\n",
        "for k in range(len(reviews)):\n",
        "  if GT[k] == classifications2[k]:\n",
        "    magnitude_results[k] = 1\n",
        "  else:\n",
        "    magnitude_results[k] = 0\n",
        "\n",
        "print(sentiment_sum2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vVk7CvDpyka",
        "outputId": "ae9d3b8e-ecdb-4d51-bc0e-7a6a164ae6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.690000\n"
          ]
        }
      ],
      "source": [
        "magnitude_results = magnitude_results# a list of binary indicators\n",
        "magnitude_accuracy = sum(magnitude_results)/len(magnitude_results)\n",
        "print(\"Accuracy: %2f\" % magnitude_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SHoGPfsAHV"
      },
      "source": [
        "#### (Q.1.3) Make a barplot of the two results (0.5pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "8LgBcYcXsEk3",
        "outputId": "31efc207-caaf-40e3-e368-e581d1b5bfa2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "# Define the accuracy values\n",
        "accuracies = [magnitude_accuracy, token_accuracy]\n",
        "labels = ['Magnitude Accuracy', 'Token Accuracy']\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(labels, accuracies, color=['blue', 'orange'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)  # Set y-axis limits from 0 to 1\n",
        "plt.title('Accuracy Comparison')\n",
        "\n",
        "# Add the value on top of the bars\n",
        "for i, v in enumerate(accuracies):\n",
        "    plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center', va='bottom')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhS8OCVxMHd"
      },
      "source": [
        "#### (Q1.4) A better threshold (1pt)\n",
        "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
        "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7gk1I-omLI"
      },
      "source": [
        "Since there are on average more positive than negative words in a given review due to the way the english language is structured it is expected that in lenghtier reviews the amount of positive words overwhelms the amount of negative words increasing the sentiment sum while on shorter reviews the amount tends to be smaller thus decreasing the sentiment sum. If a static threshold is used the afformentioned edge cases are likely to be missclassified therefore we need a new approach to deal with this problem i.e a dynamic threshold. The dynamic threshold is the product of our old static threshold with the length of the review and a constant 10/1000 (the constant is to bring the threshold value to a value similar to our static threshold). The reasoning behind this mechanism is that the threshold is analogous with the review length increasing as the length increases since in a lengthier review we expect the sentiment sum to be high and in the opposite case with small reviews we get a small threshold which accounts for the smaller sentiment sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwt0B8h8aKjr",
        "outputId": "bccc8491-e99d-4d94-ff41-01b846a042b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.675500\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "classifications2=[]\n",
        "sentiment_sum2 = []\n",
        "\n",
        "for k in range(len(reviews)):\n",
        "  sentiment_sum2.append(sum(sentiments2[k]))\n",
        "  if sum(sentiments2[k]) > (10/1000)*len(reviews_words[k]):\n",
        "    classifications2.append(1)\n",
        "  else:\n",
        "    classifications2.append(-1)\n",
        "\n",
        "magnitude_results = [None] * 2000\n",
        "for k in range(len(reviews)):\n",
        "  if GT[k] == classifications2[k]:\n",
        "    magnitude_results[k] = 1\n",
        "  else:\n",
        "    magnitude_results[k] = 0\n",
        "\n",
        "magnitude_results = magnitude_results# a list of binary indicators\n",
        "magnitude_accuracy = sum(magnitude_results)/len(magnitude_results)\n",
        "print(\"Accuracy: %2f\" % magnitude_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibV4nR89BXb"
      },
      "source": [
        "# (2) Naive Bayes (9.5pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9adQnuwia"
      },
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described by Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing texts as vectors, making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text—as the name suggests.\n",
        "\n",
        "## Writing your own classifier (4pts)\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you in a position to replicate\n",
        "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. The\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c34Yh1ZrzMvS"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "\n",
        "# Define training function\n",
        "def train_naive_bayes(reviews):\n",
        "    class_counts = Counter()\n",
        "    word_counts = {'POS': Counter(), 'NEG': Counter()}\n",
        "    vocab = set()\n",
        "\n",
        "\n",
        "    # Process each review\n",
        "    for review in reviews:\n",
        "        sentiment = review[\"sentiment\"]\n",
        "        class_counts[sentiment] += 1\n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, pos in sentence:\n",
        "                word = word.lower()\n",
        "                word_counts[sentiment][word] += 1 # Update word count for sentiment\n",
        "                vocab.add(word)\n",
        "\n",
        "    # Identify words that only appear in one class and remove them\n",
        "    single_class_words = {word for word in vocab if (word in word_counts['POS']) != (word in word_counts['NEG'])}\n",
        "    vocab = vocab - single_class_words\n",
        "\n",
        "    # Calculate priors for both classes\n",
        "    total_reviews = sum(class_counts.values())\n",
        "    class_priors = {cls: count / total_reviews for cls, count in class_counts.items()}\n",
        "\n",
        "    # Calculate word likelihoods\n",
        "    word_likelihoods = {\n",
        "        cls: {\n",
        "            word: word_counts[cls][word] / (sum(word_counts[cls].values()))\n",
        "            for word in vocab\n",
        "        }\n",
        "        for cls in class_counts\n",
        "    }\n",
        "\n",
        "    return class_priors, word_likelihoods, vocab, word_counts\n",
        "\n",
        "# Define prediction function\n",
        "def predict_naive_bayes(review, class_priors, word_likelihoods, vocab):\n",
        "    scores = {}\n",
        "\n",
        "    # For each class, compute log-probabilities\n",
        "    for cls in class_priors:\n",
        "        log_prob = math.log(class_priors[cls])\n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, pos in sentence:\n",
        "                word = word.lower()\n",
        "                if word in vocab and word_likelihoods[cls].get(word, 0)> 0:\n",
        "                    log_prob += math.log(word_likelihoods[cls][word])\n",
        "        scores[cls] = log_prob\n",
        "\n",
        "    # Select the class with the highest log-probability\n",
        "    return max(scores, key=scores.get)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpyQSBSkb33"
      },
      "source": [
        "#### (Q2.1) Unseen words (1pt)\n",
        "The presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
        "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes at test time**.  What would be the problem instead with skipping words only for one class in case 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanFiYYnoxDW"
      },
      "source": [
        "This would cause a bias towards one class in the classification process. If we keep a word for a class that has seen the word while skipping it for the other class, it would increase the log probability for the class that has seen the words. If we do this for a significant amount of words, a bias might arise potentially pointing the prediction towards the class which has seen more words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZRhaI3WvzC"
      },
      "source": [
        "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7zaJYGFvIJ3",
        "outputId": "9187e63b-710f-4825-f184-2c10487fba54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurracy:  0.825\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "train_reviews = [r for r in reviews if int(r[\"cv\"]) < 900]\n",
        "test_reviews = [r for r in reviews if int(r[\"cv\"]) > 899]\n",
        "\n",
        "class_priors_train, word_likelihoods_train, vocab_train, _ = train_naive_bayes(train_reviews)\n",
        "\n",
        "correct_predictions = 0\n",
        "for test_r in test_reviews:\n",
        "  pred = predict_naive_bayes(test_r, class_priors_train, word_likelihoods_train, vocab_train)\n",
        "  if pred == test_r[\"sentiment\"]:\n",
        "    correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_reviews)\n",
        "print(\"Acurracy: \",accuracy)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INK-PBoM6CB"
      },
      "source": [
        "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
        "\n",
        "Simulate this scenario by keeping the positive reviews\n",
        "data unchanged, but only using negative reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbcsYlipBAw"
      },
      "source": [
        "In this case, accuracy is not a good evaluation metric as it does not give us insight on how the predictions are balanced (ratio of POS/NEG predictions). A model that predicts 'POS' very often due to the imbalanced training data, can still achieve great accuracy when the test set also contains sinificantly more positive examples than negative one. The addition of a confusion matrix can help us to gain more insgiht on the model's prediction behaviour and reveal possible class imablances in the dataset. The classification accuracy dropped a bit, but is still fairly good due to the training and test set which both contain significantly more positive instances than negative ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWDkt5ZrrFGp",
        "outputId": "c3ccb517-fa29-4637-d478-1aa1ae26e02f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurracy:  0.7376237623762376\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "train_reviews = [r for r in reviews if r[\"sentiment\"] == \"POS\" or (int(r[\"cv\"]) <= 89 and r[\"sentiment\"] == \"NEG\")]\n",
        "test_reviews = [r for r in reviews if r[\"sentiment\"] == \"POS\" or (900 <= int(r[\"cv\"]) <= 909 and r[\"sentiment\"] == \"NEG\")]\n",
        "\n",
        "class_priors_train, word_likelihoods_train, vocab_train, _ = train_naive_bayes(train_reviews)\n",
        "\n",
        "correct_predictions = 0\n",
        "for test_r in test_reviews:\n",
        "  pred = predict_naive_bayes(test_r, class_priors_train, word_likelihoods_train, vocab_train)\n",
        "  if pred == test_r[\"sentiment\"]:\n",
        "    correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_reviews)\n",
        "print(\"Acurracy: \",accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJzcHX3WUDm"
      },
      "source": [
        "## Smoothing (1pt)\n",
        "\n",
        "As mentioned above, the presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ for a word\n",
        "$w_i$ becomes\n",
        "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNIcbwUWphC"
      },
      "source": [
        "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
        "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
        "Bayes classifier’s code, and report the accuracy.\n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g03yflCc9kpW",
        "outputId": "f3b664ba-53bb-4f86-ed92-25ec3143b5dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurracy:  0.825\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "def train_naive_bayes_smooth(reviews):\n",
        "    class_counts = Counter()\n",
        "    word_counts = {'POS': Counter(), 'NEG': Counter()}\n",
        "    vocab = set()\n",
        "\n",
        "\n",
        "    # Process each review\n",
        "    for review in reviews:\n",
        "        sentiment = review[\"sentiment\"]\n",
        "        class_counts[sentiment] += 1\n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, pos in sentence:\n",
        "                word = word.lower()\n",
        "                word_counts[sentiment][word] += 1 # Update word count for sentiment\n",
        "                vocab.add(word)\n",
        "\n",
        "\n",
        "    # Calculate priors for both classes\n",
        "    total_reviews = sum(class_counts.values())\n",
        "    class_priors = {cls: count / total_reviews for cls, count in class_counts.items()}\n",
        "\n",
        "    # Calculate word likelihoods with Laplace smoothing\n",
        "    vocab_size = len(vocab)\n",
        "    word_likelihoods = {\n",
        "        cls: {\n",
        "            word: (word_counts[cls][word] + 1) / (sum(word_counts[cls].values()) + vocab_size)\n",
        "            for word in vocab\n",
        "        }\n",
        "        for cls in class_counts\n",
        "    }\n",
        "\n",
        "    return class_priors, word_likelihoods, vocab\n",
        "\n",
        "# Define prediction function\n",
        "def predict_naive_bayes_smooth(review, class_priors, word_likelihoods, vocab):\n",
        "    scores = {}\n",
        "\n",
        "    # For each class, compute log-probabilities\n",
        "    for cls in class_priors:\n",
        "        log_prob = math.log(class_priors[cls])\n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, pos in sentence:\n",
        "                word = word.lower()\n",
        "                if word in vocab: # Skip words not in vocab\n",
        "                    log_prob += math.log(word_likelihoods[cls].get(word))\n",
        "        scores[cls] = log_prob\n",
        "\n",
        "    # Select the class with the highest log-probability\n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "train_reviews = [r for r in reviews if int(r[\"cv\"]) < 900]\n",
        "test_reviews = [r for r in reviews if int(r[\"cv\"]) > 899]\n",
        "\n",
        "class_priors_train, word_likelihoods_train, vocab_train = train_naive_bayes_smooth(train_reviews)\n",
        "\n",
        "correct_predictions = 0\n",
        "for test_r in test_reviews:\n",
        "  pred = predict_naive_bayes_smooth(test_r, class_priors_train, word_likelihoods_train, vocab_train)\n",
        "  if pred == test_r[\"sentiment\"]:\n",
        "    correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_reviews)\n",
        "print(\"Acurracy: \",accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGcgwba87D5"
      },
      "source": [
        "## Cross-Validation (1.5pts)\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that gets better\n",
        "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
        "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
        "time holding out one of the folds for testing, training our classifier\n",
        "on the remaining N - 1 data folds, and reporting performance on the\n",
        "held-out fold. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeLcbSauGtR"
      },
      "source": [
        "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KeCGPa7Nuzx",
        "outputId": "6ffb0ac9-7833-4244-cd4f-990ab87e5af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for fold 1: 0.7900\n",
            "Accuracy for fold 2: 0.8350\n",
            "Accuracy for fold 3: 0.8050\n",
            "Accuracy for fold 4: 0.8250\n",
            "Accuracy for fold 5: 0.7800\n",
            "Accuracy for fold 6: 0.8450\n",
            "Accuracy for fold 7: 0.8300\n",
            "Accuracy for fold 8: 0.7750\n",
            "Accuracy for fold 9: 0.8300\n",
            "Accuracy for fold 10: 0.8400\n",
            "\n",
            "Average accuracy across 10 folds: 0.8155\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "def cross_val_round_rob(reviews, num_folds = 10):\n",
        "\n",
        "  # Split reviews in 10 folds using round robin\n",
        "  folds = [[] for _ in range(num_folds)]\n",
        "  for i, review in enumerate(reviews):\n",
        "    fold_index = i % num_folds\n",
        "    folds[fold_index].append(review)\n",
        "\n",
        "  accuracies = []\n",
        "  for i in range(num_folds):\n",
        "    # Hold out one fold for testing, use the remaining folds for training\n",
        "    test_set = folds[i]\n",
        "    train_set = [review for j in range(num_folds) if j != i for review in folds[j]]\n",
        "\n",
        "    # Train the naive bayes classifier with Laplacian smoothing\n",
        "    class_priors_train, word_likelihoods_train, vocab_train = train_naive_bayes_smooth(train_set)\n",
        "\n",
        "    # Evaluate performance\n",
        "    correct_predictions = 0\n",
        "    for test_review in test_set:\n",
        "      pred = predict_naive_bayes_smooth(test_review, class_priors_train, word_likelihoods_train, vocab_train)\n",
        "      if pred == test_review[\"sentiment\"]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy per fold individually\n",
        "    fold_acc = correct_predictions / len(test_set)\n",
        "    accuracies.append(fold_acc)\n",
        "    print(f\"Accuracy for fold {i + 1}: {fold_acc:.4f}\")\n",
        "\n",
        "  # Calculate average accuracy over all folds\n",
        "  avg_acc = sum(accuracies) / num_folds\n",
        "  print(f\"\\nAverage accuracy across 10 folds: {avg_acc:.4f}\")\n",
        "  return accuracies, avg_acc\n",
        "\n",
        "# Run the cross-validation\n",
        "accuracy_list, average_accuracy = cross_val_round_rob(reviews)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otdlsDXBNyOa"
      },
      "source": [
        "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).** Note: you're not allowed to use a library for computing the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoBQm1KuNzNR",
        "outputId": "70c042b7-a343-4f00-b9bc-1d1432d7d5a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance:  0.0006022499999999986\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "variance = sum((x - average_accuracy)**2 for x in accuracy_list) / len(accuracy_list)\n",
        "print(\"Variance: \", variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A2zX9_BRKm"
      },
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKK8FNt8VtcZ"
      },
      "source": [
        "### Stemming (1.5pts)\n",
        "\n",
        "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxtCul1IrBi_"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def train_naive_bayes_stem(reviews):\n",
        "    class_counts = Counter()\n",
        "    word_counts = {'POS': Counter(), 'NEG': Counter()}\n",
        "    vocab = set()\n",
        "\n",
        "\n",
        "    # Process each review\n",
        "    for review in reviews:\n",
        "        sentiment = review[\"sentiment\"]\n",
        "        class_counts[sentiment] += 1\n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, pos in sentence:\n",
        "                word = stemmer.stem(word.lower())\n",
        "                word_counts[sentiment][word] += 1 # Update word count for sentiment\n",
        "                vocab.add(word)\n",
        "\n",
        "    # Identify words that only appear in one class and remove them\n",
        "    single_class_words = {word for word in vocab if (word in word_counts['POS']) != (word in word_counts['NEG'])}\n",
        "    vocab = vocab - single_class_words\n",
        "\n",
        "    # Calculate priors for both classes\n",
        "    total_reviews = sum(class_counts.values())\n",
        "    class_priors = {cls: count / total_reviews for cls, count in class_counts.items()}\n",
        "\n",
        "    # Calculate word likelihoods\n",
        "    word_likelihoods = {\n",
        "        cls: {\n",
        "            word: word_counts[cls][word] / (sum(word_counts[cls].values()))\n",
        "            for word in vocab\n",
        "        }\n",
        "        for cls in class_counts\n",
        "    }\n",
        "\n",
        "    return class_priors, word_likelihoods, vocab\n",
        "\n",
        "# Define prediction function\n",
        "def predict_naive_bayes_stem(review, class_priors, word_likelihoods, vocab):\n",
        "    scores = {}\n",
        "\n",
        "    # For each class, compute log-probabilities\n",
        "    for cls in class_priors:\n",
        "        log_prob = math.log(class_priors[cls])\n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, pos in sentence:\n",
        "                word = stemmer.stem(word.lower())\n",
        "                if word in vocab and word_likelihoods[cls].get(word, 0)> 0:\n",
        "                    log_prob += math.log(word_likelihoods[cls][word])\n",
        "        scores[cls] = log_prob\n",
        "\n",
        "    # Select the class with the highest log-probability\n",
        "    return max(scores, key=scores.get)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrJ1BeLXTnk"
      },
      "source": [
        "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
        "Use cross-validation to evaluate the classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYqKBOiIrInT",
        "outputId": "1bbf45d4-13f2-4530-ee1a-a22cfdb6ace7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for fold 1: 0.7750\n",
            "Accuracy for fold 2: 0.8350\n",
            "Accuracy for fold 3: 0.8050\n",
            "Accuracy for fold 4: 0.8650\n",
            "Accuracy for fold 5: 0.7850\n",
            "Accuracy for fold 6: 0.8450\n",
            "Accuracy for fold 7: 0.8100\n",
            "Accuracy for fold 8: 0.7900\n",
            "Accuracy for fold 9: 0.8300\n",
            "Accuracy for fold 10: 0.8150\n",
            "\n",
            "Average accuracy across 10 folds: 0.8155\n"
          ]
        }
      ],
      "source": [
        "# YOUR ANSWER HERE\n",
        "def cross_val_round_rob_stem(reviews, num_folds = 10):\n",
        "\n",
        "  # Split reviews in 10 folds using round robin\n",
        "  folds = [[] for _ in range(num_folds)]\n",
        "  for i, review in enumerate(reviews):\n",
        "    fold_index = i % num_folds\n",
        "    folds[fold_index].append(review)\n",
        "\n",
        "  accuracies = []\n",
        "  for i in range(num_folds):\n",
        "    # Hold out one fold for testing, use the remaining folds for training\n",
        "    test_set = folds[i]\n",
        "    train_set = [review for j in range(num_folds) if j != i for review in folds[j]]\n",
        "\n",
        "    # Train the naive bayes classifier with stemming\n",
        "    class_priors_train, word_likelihoods_train, vocab_train = train_naive_bayes_stem(train_set)\n",
        "\n",
        "    # Evaluate performance\n",
        "    correct_predictions = 0\n",
        "    for test_review in test_set:\n",
        "      pred = predict_naive_bayes_stem(test_review, class_priors_train, word_likelihoods_train, vocab_train)\n",
        "      if pred == test_review[\"sentiment\"]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy per fold individually\n",
        "    fold_acc = correct_predictions / len(test_set)\n",
        "    accuracies.append(fold_acc)\n",
        "    print(f\"Accuracy for fold {i + 1}: {fold_acc:.4f}\")\n",
        "\n",
        "  # Calculate average accuracy over all folds\n",
        "  avg_acc = sum(accuracies) / num_folds\n",
        "  print(f\"\\nAverage accuracy across 10 folds: {avg_acc:.4f}\")\n",
        "  return accuracies, avg_acc\n",
        "\n",
        "# Run the cross-validation\n",
        "accuracy_list, average_accuracy = cross_val_round_rob_stem(reviews)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDHVq_1XUVP"
      },
      "source": [
        "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA3vee5-rJyy",
        "outputId": "0a145e40-4a02-4ea1-ab85-89338c56aaf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size without stemming: 45348\n",
            "Vocabulary size with stemming: 32404\n"
          ]
        }
      ],
      "source": [
        "# Without Stemming\n",
        "def train_naive_bayes_no_stem(reviews):\n",
        "    class_counts = Counter()\n",
        "    word_counts = {'POS': Counter(), 'NEG': Counter()}\n",
        "    vocab = set()\n",
        "\n",
        "    # Process each review\n",
        "    for review in reviews:\n",
        "        sentiment = review[\"sentiment\"]\n",
        "        class_counts[sentiment] += 1\n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, pos in sentence:\n",
        "                word = word.lower()  # No stemming, use word as is\n",
        "                word_counts[sentiment][word] += 1\n",
        "                vocab.add(word)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "# With Stemming\n",
        "def train_naive_bayes_stem(reviews):\n",
        "    stemmer = PorterStemmer()\n",
        "    class_counts = Counter()\n",
        "    word_counts = {'POS': Counter(), 'NEG': Counter()}\n",
        "    vocab = set()\n",
        "\n",
        "    # Process each review\n",
        "    for review in reviews:\n",
        "        sentiment = review[\"sentiment\"]\n",
        "        class_counts[sentiment] += 1\n",
        "\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, pos in sentence:\n",
        "                word = stemmer.stem(word.lower())  # Apply stemming\n",
        "                word_counts[sentiment][word] += 1\n",
        "                vocab.add(word)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "# Running the functions\n",
        "train_reviews = [r for r in reviews if int(r[\"cv\"]) < 900]  # Train on reviews with cv < 900\n",
        "\n",
        "# Without stemming\n",
        "vocab_no_stem = train_naive_bayes_no_stem(train_reviews)\n",
        "print(f\"Vocabulary size without stemming: {len(vocab_no_stem)}\")\n",
        "\n",
        "# With stemming\n",
        "vocab_stem = train_naive_bayes_stem(train_reviews)\n",
        "print(f\"Vocabulary size with stemming: {len(vocab_stem)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoazfxbNV5Lq"
      },
      "source": [
        "### N-grams (1.5pts)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to use **n-gram** features.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjy3I7-qWiu"
      },
      "source": [
        "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
        "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrUchrjjOYhc"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function to improve efficiency\n",
        "def preprocess_ngrams(reviews, n):\n",
        "    for review in reviews:\n",
        "        all_words = [word.lower() for sentence in review[\"content\"] for word, pos in sentence]\n",
        "        review_ngrams = [[] for _ in range(n)] # Create empty list for each n-gram level\n",
        "\n",
        "        for i in range(1, n + 1):\n",
        "            ngrams_generated = [\" \".join(ngram) for ngram in ngrams(all_words, i)]\n",
        "            review_ngrams[i - 1].extend(ngrams_generated)\n",
        "\n",
        "        review[\"ngram_content\"] = review_ngrams\n",
        "\n",
        "    return reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYuKMTOpq9jz"
      },
      "outputs": [],
      "source": [
        "def train_naive_bayes_smooth_ngram(reviews):\n",
        "    class_counts = Counter()\n",
        "    word_counts = {'POS': Counter(), 'NEG': Counter()}\n",
        "    vocab = set()\n",
        "\n",
        "    # Process each review\n",
        "    for review in reviews:\n",
        "        sentiment = review[\"sentiment\"]\n",
        "        class_counts[sentiment] += 1\n",
        "\n",
        "        for ngram_list in review[\"ngram_content\"]:\n",
        "          for ngram_str in ngram_list:\n",
        "            word_counts[sentiment][ngram_str] += 1\n",
        "            vocab.add(ngram_str)\n",
        "\n",
        "    # Calculate priors for both classes\n",
        "    total_reviews = sum(class_counts.values())\n",
        "    class_priors = {cls: count / total_reviews for cls, count in class_counts.items()}\n",
        "\n",
        "    # Calculate total word counts once for each class\n",
        "    total_word_counts = {cls: sum(word_counts[cls].values()) for cls in class_counts}\n",
        "\n",
        "    # Calculate word likelihoods with Laplace smoothing\n",
        "    vocab_size = len(vocab)\n",
        "    word_likelihoods = {\n",
        "        cls: {\n",
        "            word: (word_counts[cls][word] + 1) / (total_word_counts[cls] + vocab_size)\n",
        "            for word in vocab\n",
        "        }\n",
        "        for cls in class_counts\n",
        "    }\n",
        "\n",
        "    return class_priors, word_likelihoods, vocab\n",
        "\n",
        "# Define prediction function\n",
        "def predict_naive_bayes_smooth_ngram(review, class_priors, word_likelihoods, vocab):\n",
        "    scores = {}\n",
        "\n",
        "    # Pre-compite Laplace smooting component per class for efficiency\n",
        "    smoothing_denomiator = {\n",
        "        cls: sum(word_likelihoods[cls].values()) + len(vocab)\n",
        "        for cls in class_priors\n",
        "    }\n",
        "\n",
        "    # For each class, compute log-probabilities\n",
        "    for cls in class_priors:\n",
        "        log_prob = math.log(class_priors[cls])\n",
        "\n",
        "        for ngram_list in review[\"ngram_content\"]:\n",
        "          for ngram_str in ngram_list:\n",
        "            if ngram_str in vocab:\n",
        "              log_prob += math.log(word_likelihoods[cls].get(ngram_str, 1/ smoothing_denomiator[cls]))\n",
        "        scores[cls] = log_prob\n",
        "\n",
        "    # Select the class with the highest log-probability\n",
        "    return max(scores, key=scores.get)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F53yhyxhLAjv",
        "outputId": "97affba9-7054-4d0e-dbea-2722d4323f76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model with unigrams + bigrams (n=2):\n",
            "Accuracy for fold 1: 0.7950\n",
            "Accuracy for fold 2: 0.8500\n",
            "Accuracy for fold 3: 0.8400\n",
            "Accuracy for fold 4: 0.8750\n",
            "Accuracy for fold 5: 0.8100\n",
            "Accuracy for fold 6: 0.8600\n",
            "Accuracy for fold 7: 0.8300\n",
            "Accuracy for fold 8: 0.8300\n",
            "Accuracy for fold 9: 0.8450\n",
            "Accuracy for fold 10: 0.8350\n",
            "\n",
            "Average accuracy across 10 folds: 0.8370\n",
            "\n",
            "Evaluating model with unigrams + bigrams + trigrams (n=3):\n",
            "Accuracy for fold 1: 0.7900\n",
            "Accuracy for fold 2: 0.8500\n",
            "Accuracy for fold 3: 0.8350\n",
            "Accuracy for fold 4: 0.8600\n",
            "Accuracy for fold 5: 0.8200\n",
            "Accuracy for fold 6: 0.8500\n",
            "Accuracy for fold 7: 0.8500\n",
            "Accuracy for fold 8: 0.8400\n",
            "Accuracy for fold 9: 0.8500\n",
            "Accuracy for fold 10: 0.8050\n",
            "\n",
            "Average accuracy across 10 folds: 0.8350\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "def cross_val_round_rob_ngram(reviews, num_folds = 10):\n",
        "  # Split reviews into 10 folds using round robin\n",
        "  folds = [[] for _ in range(num_folds)]\n",
        "  for i, review in enumerate(reviews):\n",
        "    fold_index = i % num_folds\n",
        "    folds[fold_index].append(review)\n",
        "\n",
        "  accuracies = []\n",
        "  for i in range(num_folds):\n",
        "    # Hold out one fold for testing, use the remaining folds for training\n",
        "    test_set = folds[i]\n",
        "    train_set = [review for j in range(num_folds) if j != i for review in folds[j]]\n",
        "\n",
        "    # Train the naive bayes classifier with Laplacian smoothing\n",
        "    class_priors_train, word_likelihoods_train, vocab_train = train_naive_bayes_smooth_ngram(train_set)\n",
        "\n",
        "    # Evaluate performance\n",
        "    correct_predictions = 0\n",
        "    for test_review in test_set:\n",
        "      pred = predict_naive_bayes_smooth_ngram(test_review, class_priors_train, word_likelihoods_train, vocab_train)\n",
        "      if pred == test_review[\"sentiment\"]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy per fold individually\n",
        "    fold_acc = correct_predictions / len(test_set)\n",
        "    accuracies.append(fold_acc)\n",
        "    print(f\"Accuracy for fold {i + 1}: {fold_acc:.4f}\")\n",
        "\n",
        "  # Calculate average accuracy over all folds\n",
        "  avg_acc = sum(accuracies) / num_folds\n",
        "  print(f\"\\nAverage accuracy across {num_folds} folds: {avg_acc:.4f}\")\n",
        "  return accuracies, avg_acc\n",
        "\n",
        "# Preprocess reviews for unigrams + bigrams (n=2)\n",
        "print(\"Evaluating model with unigrams + bigrams (n=2):\")\n",
        "reviews_n2 = preprocess_ngrams(reviews, n=2)\n",
        "acc_list_n2, avg_acc_n2 = cross_val_round_rob_ngram(reviews_n2)\n",
        "\n",
        "# Preprocess reviews for unigrams + bigrams + trigrams (n=3)\n",
        "print(\"\\nEvaluating model with unigrams + bigrams + trigrams (n=3):\")\n",
        "reviews_n3 = preprocess_ngrams(reviews, n=3)\n",
        "acc_list_n3, avg_acc_n3 = cross_val_round_rob_ngram(reviews_n3)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGGArkrWoL"
      },
      "source": [
        "\n",
        "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q2.8)?\n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGZ9SV8pPaa"
      },
      "source": [
        "Theoretically we would expect the number of features to increase conform the following pattern:\n",
        "- Unigrams: $V$\n",
        "- Bigrams: $V^2$\n",
        "- Trigrams: $V^3$\n",
        "\n",
        "Where in our case V is defined by the length of the vocab.\n",
        "\n",
        "\n",
        "This means that the number of features of 2.8 (which are just the unigrams) would just be $V$, the number of features of unigrams + bigrams would be $V + V^2$ and the number of features for the unigrams + bigrams + trigrams $V + V^2 + V^3$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z8sAJeUrdtM",
        "outputId": "83a4dd71-d8bd-4109-c33b-8c6e91dc986b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features for unigrams + bigrams: 471032\n",
            "Number of features for unigrams + bigrams + trigrams: 1416686\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "train_reviews = [r for r in reviews if int(r[\"cv\"]) < 900]  # Train on reviews with cv < 900\n",
        "\n",
        "# Uni- + Bi- + Trigrams\n",
        "reviews_n2 = preprocess_ngrams(train_reviews, n=2)\n",
        "_, _, vocab_n2 = train_naive_bayes_smooth_ngram(reviews_n2)\n",
        "print(\"Number of features for unigrams + bigrams:\", len(vocab_n2))\n",
        "\n",
        "# Uni- + Bigrams\n",
        "reviews_n3 = preprocess_ngrams(train_reviews, n=3)\n",
        "_, _, vocab_n3 = train_naive_bayes_smooth_ngram(reviews_n3)\n",
        "print(\"Number of features for unigrams + bigrams + trigrams:\", len(vocab_n3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWKDL3YV6vh"
      },
      "source": [
        "# (3) Support Vector Machines (4pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSYhcVaoJGt"
      },
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of\n",
        "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LnzNtQBV8gr"
      },
      "source": [
        "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier with smoothing.\n",
        "Use cross-validation to evaluate the performance of the classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBscui8Mvoz0"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "train_reviews = [r for r in reviews if int(r[\"cv\"]) < 900]\n",
        "test_reviews = [r for r in reviews if int(r[\"cv\"]) > 899]\n",
        "\n",
        "# _, _, _, word_counts = train_naive_bayes(train_reviews)\n",
        "# print(word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf0xu2H4NSl8"
      },
      "outputs": [],
      "source": [
        "reviews_words = []\n",
        "for review in reviews:\n",
        "  review_words = []\n",
        "  for sentence in review[\"content\"]:\n",
        "    review_words.extend([word for word, tag in sentence])\n",
        "  reviews_words.append(review_words)\n",
        "\n",
        "all_words = [word for review in reviews_words for word in review]\n",
        "\n",
        "unique_review_words = list(set(all_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHzJjEF4WR6X"
      },
      "outputs": [],
      "source": [
        "train_words = []\n",
        "for review in train_reviews:\n",
        "  review_words = []\n",
        "  for sentence in review[\"content\"]:\n",
        "    review_words.extend([word for word, tag in sentence])\n",
        "  train_words.append(review_words)\n",
        "\n",
        "test_words = []\n",
        "for review in test_reviews:\n",
        "  review_words = []\n",
        "  for sentence in review[\"content\"]:\n",
        "    review_words.extend([word for word, tag in sentence])\n",
        "  test_words.append(review_words)\n",
        "\n",
        "\n",
        "GT_SVM = []\n",
        "for i, r in enumerate(train_reviews):\n",
        "    sentiment = r.get(\"sentiment\")\n",
        "    if sentiment == \"POS\":\n",
        "        GT_SVM.append(1)\n",
        "    elif sentiment == \"NEG\":\n",
        "        GT_SVM.append(-1)\n",
        "\n",
        "GT_SVM_test = []\n",
        "for i, r in enumerate(test_reviews):\n",
        "    sentiment = r.get(\"sentiment\")\n",
        "    if sentiment == \"POS\":\n",
        "        GT_SVM_test.append(1)\n",
        "    elif sentiment == \"NEG\":\n",
        "        GT_SVM_test.append(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e9Kr18VaARz"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "\n",
        "#Create the vocabulary from combined word counts of POS and NEG\n",
        "# Using a set union to ensure each word appears only once in the vocabulary.\n",
        "vocabulary = {word: idx for idx, word in enumerate(unique_review_words)}\n",
        "\n",
        "#Initialize an empty matrix to hold the word counts for each review\n",
        "#The number of rows is the number of reviews, and the number of columns is the vocabulary size\n",
        "feature_matrix = np.zeros((len(train_words), len(vocabulary)), dtype=int)\n",
        "\n",
        "#Populate the feature matrix\n",
        "for i, review in enumerate(train_words):\n",
        "    for word in review:\n",
        "        if word in vocabulary:  # Only add if the word is in the vocabulary\n",
        "            word_index = vocabulary[word]\n",
        "            feature_matrix[i, word_index] += 1\n",
        "\n",
        "# Convert to a sparse matrix format to save memory\n",
        "feature_matrix_sparse = sparse.csr_matrix(feature_matrix)\n",
        "\n",
        "\n",
        "\n",
        "##TEST\n",
        "feature_matrix_test = np.zeros((len(test_words), len(vocabulary)), dtype=int)\n",
        "\n",
        "#Populate the feature matrix\n",
        "for i, review in enumerate(test_words):\n",
        "    for word in review:\n",
        "        if word in vocabulary:  # Only add if the word is in the vocabulary\n",
        "            word_index = vocabulary[word]\n",
        "            feature_matrix_test[i, word_index] += 1\n",
        "\n",
        "# Convert to a sparse matrix format to save memory\n",
        "feature_matrix_sparse_test = sparse.csr_matrix(feature_matrix_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILHsOhVFbuap",
        "outputId": "183ebfe0-d734-46f4-eff8-60c6a45d6a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated accuracies for each fold (training): [0.84444444 0.78333333 0.83888889 0.85555556 0.85555556 0.8\n",
            " 0.85       0.86666667 0.78333333 0.86666667]\n",
            "Mean accuracy of SVM: 83.44%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Initialize the LinearSVC model\n",
        "svm_model = LinearSVC(max_iter=50000)\n",
        "# Perform 10-fold cross-validation and capture accfeature_matrix_sparseuracy for each fold\n",
        "accuracies = cross_val_score(svm_model, feature_matrix_sparse, GT_SVM, cv=10, scoring='accuracy')\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "print(\"Cross-validated accuracies for each fold (training):\", accuracies)\n",
        "print(\"Mean accuracy of SVM: {:.2f}%\".format(np.mean(accuracies) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpe6VS5SlBVV",
        "outputId": "f9424edf-2775-4359-a4a8-17268927716a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of SVM: 86.00%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Initialize the LinearSVC model\n",
        "svm_model = LinearSVC(max_iter=50000)\n",
        "\n",
        "# Train the model on the full training data\n",
        "svm_model.fit(feature_matrix_sparse, GT_SVM)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_test = svm_model.predict(feature_matrix_test)\n",
        "\n",
        "# Calculate and print the test accuracy\n",
        "test_accuracy = accuracy_score(GT_SVM_test, predictions_test)\n",
        "print(f\"Test Accuracy of SVM: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDolIGV00VgT",
        "outputId": "a05b2704-bd91-4ce8-df62-ad289a4337b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated accuracies for each fold: [0.8        0.81666667 0.8        0.83333333 0.83888889 0.77222222\n",
            " 0.82777778 0.83888889 0.81111111 0.77777778]\n",
            "Mean accuracy of Naive Bayes: 81.17%\n",
            "Test Accuracy of Naive Bayes: 82.50%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Initialize the Naive Bayes model with smoothing (alpha=1 for Laplace smoothing)\n",
        "nb_model = MultinomialNB(alpha=1.0)\n",
        "\n",
        "# Perform 10-fold cross-validation and capture accuracy for each fold\n",
        "accuracies = cross_val_score(nb_model, feature_matrix_sparse, GT_SVM, cv=10, scoring='accuracy')\n",
        "\n",
        "# Calculate and print the cross-validated accuracies for each fold\n",
        "print(\"Cross-validated accuracies for each fold:\", accuracies)\n",
        "print(\"Mean accuracy of Naive Bayes: {:.2f}%\".format(np.mean(accuracies) * 100))\n",
        "\n",
        "# After cross-validation, you can train on the full dataset and evaluate on the test set:\n",
        "nb_model.fit(feature_matrix_sparse, GT_SVM)  # Train the Naive Bayes model on the full training data\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_test = nb_model.predict(feature_matrix_test)\n",
        "\n",
        "# Calculate and print the test accuracy\n",
        "test_accuracy = accuracy_score(GT_SVM_test, predictions_test)\n",
        "print(f\"Test Accuracy of Naive Bayes: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXVWcK0V9qY"
      },
      "source": [
        "### POS disambiguation (2pts)\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
        "replicate the results obtained by Pang et al. (2002).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3I82o4oWGu"
      },
      "source": [
        "####(Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOvjYe-t2Br6"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Initialize dictionaries and lists\n",
        "review_word_tags = {}\n",
        "unique_word_tags = []\n",
        "\n",
        "# Processing reviews\n",
        "for review in reviews:\n",
        "    for sentence in review[\"content\"]:\n",
        "        for word, tag in sentence:\n",
        "            word_tag = f\"{word}_{tag}\"  # Combine word and tag\n",
        "            review_word_tags[word_tag] = tag  # Store word_tag with tag\n",
        "            unique_word_tags.append(word_tag)  # Add to unique list\n",
        "\n",
        "# Remove duplicates in unique_word_tags\n",
        "unique_word_tags = list(set(unique_word_tags))\n",
        "\n",
        "train_word_tags = []\n",
        "# Processing train_reviews\n",
        "for review in train_reviews:\n",
        "    train_review_word_tags = []  # List for this review's word_tag pairs\n",
        "\n",
        "    for sentence in review[\"content\"]:\n",
        "        for word, tag in sentence:\n",
        "            word_tag = f\"{word}_{tag}\"  # Combine word and tag\n",
        "            if word_tag not in train_review_word_tags:\n",
        "                train_review_word_tags.append(word_tag)  # Store word_tag pair\n",
        "\n",
        "    # Remove duplicates and add to main list\n",
        "    train_word_tags.append(list(set(train_review_word_tags)))\n",
        "\n",
        "test_word_tags = []\n",
        "# Processing test_reviews\n",
        "for review in test_reviews:\n",
        "    test_review_word_tags = []  # List for this review's word_tag pairs\n",
        "\n",
        "    for sentence in review[\"content\"]:\n",
        "        for word, tag in sentence:\n",
        "            word_tag = f\"{word}_{tag}\"  # Combine word and tag\n",
        "            if word_tag not in test_review_word_tags:\n",
        "                test_review_word_tags.append(word_tag)  # Store word_tag pair\n",
        "\n",
        "    # Remove duplicates and add to main list\n",
        "    test_word_tags.append(list(set(test_review_word_tags)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce51PRPDWKCZ"
      },
      "outputs": [],
      "source": [
        "#Create the vocabulary_tagged from combined word counts of POS and NEG\n",
        "# Using a set union to ensure each word appears only once in the vocabulary_tagged.\n",
        "vocabulary_tagged = {word: idx for idx, word in enumerate(unique_word_tags)}\n",
        "\n",
        "#Initialize an empty matrix to hold the word counts for each review\n",
        "#The number of rows is the number of reviews, and the number of columns is the vocabulary_tagged size\n",
        "feature_matrix_tagged = np.zeros((len(train_word_tags), len(vocabulary_tagged)), dtype=int)\n",
        "\n",
        "#Populate the feature matrix\n",
        "for i, review in enumerate(train_word_tags):\n",
        "    for word in review:\n",
        "        if word in vocabulary_tagged:  # Only add if the word is in the vocabulary_tagged\n",
        "            word_index = vocabulary_tagged[word]\n",
        "            feature_matrix_tagged[i, word_index] += 1\n",
        "\n",
        "# Convert to a sparse matrix format to save memory\n",
        "feature_matrix_tagged_sparse = sparse.csr_matrix(feature_matrix_tagged)\n",
        "\n",
        "\n",
        "\n",
        "##TEST\n",
        "feature_matrix_tagged_test = np.zeros((len(test_word_tags), len(vocabulary_tagged)), dtype=int)\n",
        "\n",
        "#Populate the feature matrix\n",
        "for i, review in enumerate(test_word_tags):\n",
        "    for word in review:\n",
        "        if word in vocabulary_tagged:  # Only add if the word is in the vocabulary_tagged\n",
        "            word_index = vocabulary_tagged[word]\n",
        "            feature_matrix_tagged_test[i, word_index] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYVn-7hMXOIN",
        "outputId": "00a34173-6dee-404b-afea-193b835a7ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated accuracies for each fold (training): [0.85       0.80555556 0.83333333 0.86111111 0.86666667 0.82222222\n",
            " 0.88333333 0.83888889 0.86111111 0.84444444]\n",
            "Mean accuracy of SVM: 84.67%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the LinearSVC model\n",
        "svm_model = LinearSVC(max_iter=50000)\n",
        "# Perform 10-fold cross-validation and capture accfeature_matrix_sparseuracy for each fold\n",
        "accuracies = cross_val_score(svm_model, feature_matrix_tagged_sparse, GT_SVM, cv=10, scoring='accuracy')\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "print(\"Cross-validated accuracies for each fold (training):\", accuracies)\n",
        "print(\"Mean accuracy of SVM: {:.2f}%\".format(np.mean(accuracies) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USjuk4YBXaF4",
        "outputId": "aaf43937-c6fb-43e0-c0df-08aaad958c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of SVM: 88.50%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the LinearSVC model\n",
        "svm_model = LinearSVC(max_iter=50000)\n",
        "\n",
        "# Train the model on the full training data\n",
        "svm_model.fit(feature_matrix_tagged_sparse, GT_SVM)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_test_3 = svm_model.predict(feature_matrix_tagged_test)\n",
        "\n",
        "# Calculate and print the test accuracy\n",
        "test_accuracy = accuracy_score(GT_SVM_test, predictions_test_3)\n",
        "print(f\"Test Accuracy of SVM: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dt_oQupUNe"
      },
      "source": [
        "The improvement in both cross-validated accuracy and test accuracy compared to the Q3.1's SVM suggests that POS tags contribute valuable information. POS tags provide syntactic context, helping the classifier understand the grammatical function of each word, enabling better differentiation between classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-3w87eMW0w"
      },
      "source": [
        "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCUPlPozCYUX"
      },
      "outputs": [],
      "source": [
        "# List of allowed tags\n",
        "allowed_tags = {\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"}\n",
        "\n",
        "reviews_wordtags = {}\n",
        "remaining_words = []  # List to store words with tags that are not in the allowed list\n",
        "sanitized_dictionary = []\n",
        "\n",
        "for review in reviews:\n",
        "    for sentence in review[\"content\"]:\n",
        "        for word, tag in sentence:\n",
        "            if tag in allowed_tags and word not in reviews_wordtags:\n",
        "                reviews_wordtags[word] = tag  # Store word with allowed tag\n",
        "                sanitized_dictionary.append(word)\n",
        "\n",
        "train_words_sanitized = []\n",
        "# Processing train_words\n",
        "for review in train_reviews:\n",
        "    train_words_sanitized_rev = []  # List for this review's allowed words\n",
        "\n",
        "    for sentence in review[\"content\"]:\n",
        "        for word, tag in sentence:\n",
        "            if tag in allowed_tags and word not in train_words_sanitized_rev:\n",
        "                train_words_sanitized_rev.append(word)  # Store word with allowed tag\n",
        "\n",
        "    train_words_sanitized.append(train_words_sanitized_rev)\n",
        "\n",
        "test_words_sanitized = []\n",
        "# Processing test_words\n",
        "for review in test_reviews:\n",
        "    test_words_sanitized_rev = []  # List for this review's allowed words\n",
        "\n",
        "    for sentence in review[\"content\"]:\n",
        "        for word, tag in sentence:\n",
        "            if tag in allowed_tags and word not in test_words_sanitized_rev:\n",
        "                test_words_sanitized_rev.append(word)  # Store word with allowed tag\n",
        "\n",
        "    test_words_sanitized.append(test_words_sanitized_rev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z6hFVPk5mIO"
      },
      "outputs": [],
      "source": [
        "#Create the vocabulary_sanitized from combined word counts of POS and NEG\n",
        "# Using a set union to ensure each word appears only once in the vocabulary_sanitized.\n",
        "vocabulary_sanitized = {word: idx for idx, word in enumerate(sanitized_dictionary)}\n",
        "\n",
        "#Initialize an empty matrix to hold the word counts for each review\n",
        "#The number of rows is the number of reviews, and the number of columns is the vocabulary_sanitized size\n",
        "sanitized_feature_matrix = np.zeros((len(train_words_sanitized), len(vocabulary_sanitized)), dtype=int)\n",
        "\n",
        "#Populate the feature matrix\n",
        "for i, review in enumerate(train_words_sanitized):\n",
        "    for word in review:\n",
        "        if word in vocabulary_sanitized:  # Only add if the word is in the vocabulary_sanitized\n",
        "            word_index = vocabulary_sanitized[word]\n",
        "            sanitized_feature_matrix[i, word_index] += 1\n",
        "\n",
        "# Convert to a sparse matrix format to save memory\n",
        "sanitized_feature_matrix_sparse = sparse.csr_matrix(sanitized_feature_matrix)\n",
        "\n",
        "\n",
        "\n",
        "##TEST\n",
        "sanitized_feature_matrix_test = np.zeros((len(test_words_sanitized), len(vocabulary_sanitized)), dtype=int)\n",
        "\n",
        "#Populate the feature matrix\n",
        "for i, review in enumerate(test_words_sanitized):\n",
        "    for word in review:\n",
        "        if word in vocabulary_sanitized:  # Only add if the word is in the vocabulary_sanitized\n",
        "            word_index = vocabulary_sanitized[word]\n",
        "            sanitized_feature_matrix_test[i, word_index] += 1\n",
        "\n",
        "# Convert to a sparse matrix format to save memory\n",
        "sanitized_feature_matrix_sparse_test = sparse.csr_matrix(sanitized_feature_matrix_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RWm1T318jM4",
        "outputId": "797d4190-3791-4fca-bd8e-b1daebfd91da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated accuracies for each fold (training): [0.83888889 0.80555556 0.81666667 0.83333333 0.86666667 0.84444444\n",
            " 0.88888889 0.85555556 0.87777778 0.85      ]\n",
            "Mean accuracy of SVM: 84.78%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the LinearSVC model\n",
        "svm_model = LinearSVC(max_iter=50000)\n",
        "# Perform 10-fold cross-validation and capture accfeature_matrix_sparseuracy for each fold\n",
        "accuracies = cross_val_score(svm_model, sanitized_feature_matrix_sparse, GT_SVM, cv=10, scoring='accuracy')\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "print(\"Cross-validated accuracies for each fold (training):\", accuracies)\n",
        "print(\"Mean accuracy of SVM: {:.2f}%\".format(np.mean(accuracies) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSVGopYTCc8R",
        "outputId": "9a3599cf-7a20-41bc-e069-73172a77e800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of SVM: 90.00%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the LinearSVC model\n",
        "svm_model = LinearSVC(max_iter=50000)\n",
        "\n",
        "# Train the model on the full training data\n",
        "svm_model.fit(sanitized_feature_matrix_sparse, GT_SVM)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_test_3 = svm_model.predict(sanitized_feature_matrix_test)\n",
        "\n",
        "# Calculate and print the test accuracy\n",
        "test_accuracy = accuracy_score(GT_SVM_test, predictions_test_3)\n",
        "print(f\"Test Accuracy of SVM: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaxCVrs8pWSp"
      },
      "source": [
        "Removing the uninformative tokes we sligthly increase the accuracy of our model. This is because the model does not learn from generic widely used tokens in common language tokens these are uninfromative tokens and keeping only the informative ones increases the model's ability to distrinct positive from negative reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwqOciAl2No"
      },
      "source": [
        "# (4) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuse5WLmekZ"
      },
      "source": [
        "*Write your answer here in up to 500 words (-0.25pt for >50 extra words, -0.5 points for >100 extra words, ...)*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru9c0wNKM1Ck"
      },
      "source": [
        "The Lexicon-based approach, which offers a straightforward yet effective starting point. This model operates by evaluating each word in the text as either positive or negative, then summing these values to determine the sentiment of the entire document. While this simplicity makes the model efficient, it also limits its ability to capture the subtleties of language. The Lexicon model does not account for the context in which words appear or their frequency, which means it struggles with more complex sentiment expressions, leading to less accurate predictions in nuanced cases.\n",
        "\n",
        "\n",
        "In the section of the Naive Bayes classifier we experimented with different approaches looking for any improvement upon the ‘basic’ Naive Bayes classifier defined in the first part of the section. The addition of Laplace smoothing did not improve the performance, in fact the achieved accuracy without smoothing is the same as with smoothing. This might be due to the fact that the dataset likely had only a few unseen words that would have benefited from Laplace smoothing. In our case, stemming the words also did not seem to improve the performance of the classifier. This might indicate that the model can already capture important features without using stemming. The ngrams-based classifier did however seem to improve the accuracy slightly. This indicates that the classifier does benefit from the contextual relationships between the words. The classifier using unigrams + bigrams achieved a little bit higher than the unigrams + bigrams + trigrams classifier.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In our experiments, the SVM outperformed Naive Bayes in sentiment analysis with basic word features likely due to SVM's strength in finding optimal decision boundaries in high-dimensional spaces. Initially, we trained the SVM classifier with basic word features, then enhanced it by adding POS tags, and finally focused only on content words (nouns, verbs, adjectives, adverbs). Each adjustment improved performance, with the final model (content words + POS) achieving the highest test accuracy.\n",
        "\n",
        "\n",
        "These results indicate that SVM benefits significantly from focused, sentiment-relevant features. Content words offer essential sentiment cues, where nouns and adjectives reveal subjects and opinions, while verbs and adverbs convey intensity. Adding POS tags further improved performance by clarifying word functions, which helped reduce feature ambiguity. Excluding closed-class words, like prepositions and conjunctions, reduced noise, allowing the classifier to focus on the most relevant aspects of sentiment.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
